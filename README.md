# the-tale-of-llm-and-vlms
in depth exploration of llm and vlms.(notes)



1. makemore by [Andrej Karpathy](https://youtu.be/PaCmpygFfXo?si=-4KCpYbNTVHrU5j5)
2. minbpe by [karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)
3. attention? attention! [Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention/)
4.  gpt-2 again by [karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)
5. llama3 from scratch by [naklecha](https://github.com/naklecha/llama3-from-scratch)
6. llm training in simple, raw by c/cuda [karpathy](https://github.com/karpathy/llm.c)
7. decoding strategies in large language models  [mlabonne](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)
8. how to make llms go fast by [vgel](https://vgel.me/posts/faster-inference/#KV_caching)
9. a visual guide to quantization [maarten](https://www.maartengrootendorst.com/blog/quantization/)
10. extending the RoPE by [eleutherai](https://blog.eleuther.ai/yarn/)
11. the novice's llm training guide by [alpin](https://rentry.org/llm-training)
12. a survey on evaluation of large language models [paper](https://arxiv.org/abs/2307.03109)
13. flash attention by [Aleksa GordiÄ‡](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
14. mixture of experts explained [huggingface](https://huggingface.co/blog/moe)
15. vision transformer by [aman-arora](https://amaarora.github.io/posts/2021-01-18-ViT.html)
16. clip, siglip and paligemma by [umar-jamil](https://www.youtube.com/watch?v=vAmKB7iPkWw&t=12911s)

project to build along side : image captioning (vlm) 
