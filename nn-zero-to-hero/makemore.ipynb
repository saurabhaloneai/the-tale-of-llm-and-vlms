{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> this notebook contain the solution of the exercises from the video \"The spelled-out intro to language modeling: building makemore\" by [Andrej Karpathy](https://youtu.be/PaCmpygFfXo?si=-Rvr6JoZFtCKS21x) and where he build the bigram model using simple counting(sampling over prob distribution) and neural net. (5 exercises)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **E01: train a `trigram language model`, i.e. take `two characters as an input to predict the 3rd one`. Feel free to use either `counting` or a `neural net`. Evaluate the loss; Did it improve over a bigram model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you don't have any idea about N-grams model then i highly recommend you to go through this [stanford-notes](https://web.stanford.edu/~jurafsky/slp3/3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data \n",
    "\n",
    "data = open('names.txt', 'r').read().splitlines()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15   2\n"
     ]
    }
   ],
   "source": [
    "# checking the max and min lenght of the words in data\n",
    "out = []\n",
    "for w in range(len(data)):\n",
    "    out.append(len(data[w]))\n",
    "print(max(out),\" \", min(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', 'm', 'm')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0],data[0][1:2], data[0][2:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "\n",
    "for w in data:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 , ch3 in zip(chs, chs[1:], chs[2:] ):\n",
    "        tigram = (ch1, ch2, ch3)\n",
    "        t[tigram] = t.get(tigram,0) + 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(t.items(), key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " '<S>': 26,\n",
       " '<E>': 27}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(set(''.join(data)))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "stoi['<S>'] = 26\n",
    "stoi['<E>'] = 27\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the <E> and <S>.\n",
    "\n",
    "chars = sorted(set(''.join(data)))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want 3d array (cuause x1, x2 -> y)\n",
    "import torch \n",
    "N = torch.zeros((27,27,27),dtype=torch.int32)\n",
    "for w in data:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 , ch3 in zip(chs, chs[1:], chs[2:] ):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1,ix2,ix3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P = P / P.sum(2, keepdims=True) # we need to get probablibility .. so that's why normalizing the two dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[1, 1].sum() # sum of porb should be 1 then we are right in what we do above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..ce.\n",
      "..za.\n",
      "..zogh.\n",
      "..uriana.\n",
      "..kaydnevonimittain.\n",
      "..luwak.\n",
      "..ka.\n",
      "..da.\n",
      "..samiyah.\n",
      "..javer.\n"
     ]
    }
   ],
   "source": [
    "# sampling from new probs\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "  out = ['.', '.']\n",
    "  while True:\n",
    "    p = P[stoi[out[-2]], stoi[out[-1]]] # cause we need last two char to predict new \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-410414.9688\n",
      "tensor(2.0927)\n"
     ]
    }
   ],
   "source": [
    "# loss \n",
    "\n",
    "log_likehood = 0.0\n",
    "n = 0\n",
    "for w in data:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    n+=1\n",
    "    logprob = torch.log(P[ix1, ix2, ix3])\n",
    "    log_likehood += logprob\n",
    "\n",
    "print(f'{log_likehood:.4f}')\n",
    "nll = -log_likehood\n",
    "\n",
    "print(nll / n) # well loss is improved from the bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196113\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in data:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2))# two input again  \n",
    "    ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(ys.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W =  torch.randn((27*2, 27), generator=g, requires_grad=True) # 27*2 two diff weight for two inputs ix1 and ix2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.283792018890381\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for k in range(1000):\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() \n",
    "  \n",
    " \n",
    "  logits = xenc.view(-1, 27*2) @ W \n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() \n",
    "\n",
    "  W.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  W.data += -3 * W.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **E02: split up the dataset randomly into `80% train set, 10% dev set, 10% test set`. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "lenght = len(data)\n",
    "\n",
    "train_data = data[:math.floor(lenght * 0.8)]\n",
    "dev_data = data[math.floor(lenght*0.8):math.floor(lenght*0.9)]\n",
    "test_data = data[math.floor(lenght*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156850\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in train_data:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1, ix2)) \n",
    "    ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(ys.shape[0])\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W =  torch.randn((27*2, 27), generator=g, requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.281895637512207\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for k in range(1000):\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() \n",
    "  \n",
    " \n",
    "  logits = xenc.view(-1, 27*2) @ W \n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() \n",
    "\n",
    "  W.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  W.data += -3 * W.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2946)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = []\n",
    "for w in test_data:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "      \n",
    "    xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss.append(-probs[0, ix3].log())\n",
    "print(torch.tensor(loss).mean()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss on dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2705)\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for w in dev_data:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "      \n",
    "    xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27*2) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss.append(-probs[0, ix3].log())\n",
    "print(torch.tensor(loss).mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-05 : 2.2819204330444336\n",
      "alpha: 1e-05 : 2.289590835571289\n",
      "\n",
      "alpha: 0.0001 : 2.281435966491699\n",
      "alpha: 0.0001 : 2.2894060611724854\n",
      "\n",
      "alpha: 0.001 : 2.2805747985839844\n",
      "alpha: 0.001 : 2.2872159481048584\n",
      "\n",
      "alpha: 0.01 : 2.288482904434204\n",
      "alpha: 0.01 : 2.2868690490722656\n",
      "\n",
      "alpha: 0.1 : 2.3399317264556885\n",
      "alpha: 0.1 : 2.2893760204315186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    W =  torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "    for k in range(1000):\n",
    "      xenc = F.one_hot(xs, num_classes=27).float()\n",
    "      logits = xenc.view(-1, 27*2) @ W\n",
    "      counts = logits.exp()\n",
    "      probs = counts / counts.sum(1, keepdims=True)\n",
    "      loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + alpha*(W**2).mean()\n",
    "      W.grad = None\n",
    "      loss.backward()\n",
    "      W.data += -3 * W.grad\n",
    "    print(f\"alpha: {alpha} : {loss.item()}\")\n",
    "\n",
    "    loss = []\n",
    "    for w in dev_data:\n",
    "      chs = ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "          \n",
    "        xenc = F.one_hot(torch.tensor((ix1, ix2)), num_classes=27).float()\n",
    "        logits = xenc.view(-1, 27*2) @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss.append(-probs[0, ix3].log())\n",
    "    print(f\"alpha: {alpha} : {torch.tensor(loss).mean()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### result is improving .. it is saving model from on getting overfit on train_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5359888076782227\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xs, ys = [], []\n",
    "for w in data:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))  #\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27 * 27, 27), generator=g, requires_grad=True)  \n",
    "\n",
    "# train loop\n",
    "for k in range(1000):\n",
    "   \n",
    "    indices = xs[:, 0] * 27 + xs[:, 1]  # to remove f.one_hot\n",
    "\n",
    "    # lookup rows of w using combined indices\n",
    "    logits = W[indices]  # logits has shape [examplesCount, 27]\n",
    "\n",
    "    # cal probabilities\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "    \n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean()\n",
    "\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    W.data += -3 * W.grad # we chaneg this '-3'\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.291670322418213\n",
      "2.291670322418213\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in data:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))  #\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)  \n",
    "\n",
    "\n",
    "for k in range(1000):\n",
    "    \n",
    "    xenc = F.one_hot(xs, num_classes=27).float() \n",
    "    logits = xenc.view(-1, 54) @ W  \n",
    "\n",
    "\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "  \n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01 * (W ** 2).mean()\n",
    "\n",
    "   \n",
    "    cross_entropy_loss = F.cross_entropy(logits, ys) + 0.01 * (W ** 2).mean()\n",
    "\n",
    "   \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    W.data += -3 * W.grad\n",
    "\n",
    "print(loss.item())\n",
    "print(cross_entropy_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> bye !! 🐳"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
